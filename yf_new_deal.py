# -*- coding: utf-8 -*-
"""yf_new_deal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19aKTkHMNOnt5yiIiu-YYRVnCSWei855f
"""

stocklist = ['AT0000A0E9W5',
 'DE0005089031',
 'DE0005093108',
 'DE0005102008',
 'DE0005104400',
 'DE0005140008',
 'DE0005158703',
 'DE0005190003',
 'DE0005194062',
 'DE0005200000',
 'DE0005313704',
 'DE0005403901',
 'DE0005408116',
 'DE0005419105',
 'DE0005439004',
 'DE0005470306',
 'DE0005470405',
 'DE0005493365',
 'DE0005545503',
 'DE0005550636',
 'DE0005552004',
 'DE0005557508',
 'DE0005565204',
 'DE0005659700',
 'DE0005664809',
 'DE0005772206',
 'DE0005773303',
 'DE0005785604',
 'DE0005785802',
 'DE0005800601',
 'DE0005810055',
 'DE0005909006',
 'DE0006047004',
 'DE0006048432',
 'DE0006062144',
 'DE0006070006',
 'DE0006083405',
 'DE0006095003',
 'DE0006200108',
 'DE0006202005',
 'DE0006219934',
 'DE0006231004',
 'DE0006305006',
 'DE0006335003',
 'DE0006452907',
 'DE0006599905',
 'DE0006602006',
 'DE0006632003',
 'DE0006766504',
 'DE0006916604',
 'DE0006969603',
 'DE0007010803',
 'DE0007030009',
 'DE0007037129',
 'DE0007074007',
 'DE0007100000',
 'DE0007164600',
 'DE0007165631',
 'DE0007231326',
 'DE0007236101',
 'DE0007257503',
 'DE0007274136',
 'DE0007276503',
 'DE0007297004',
 'DE0007314007',
 'DE0007446007',
 'DE0007461006',
 'DE0007480204',
 'DE0007493991',
 'DE0007500001',
 'DE0007664039',
 'DE0008019001',
 'DE0008232125',
 'DE0008303504',
 'DE0008402215',
 'DE0008404005',
 'DE0008430026',
 'DE000A0D9PT0',
 'DE000A0DJ6J9',
 'DE000A0HN5C6',
 'DE000A0JBPG2',
 'DE000A0JL9W6',
 'DE000A0LD6E6',
 'DE000A0TGJ55',
 'DE000A0WMPJ6',
 'DE000A0Z23Q5',
 'DE000A0Z2ZZ5',
 'DE000A12DM80',
 'DE000A13SX22',
 'DE000A161408',
 'DE000A161N30',
 'DE000A1DAHH0',
 'DE000A1EWWW0',
 'DE000A1H8BV3',
 'DE000A1J5RX9',
 'DE000A1ML7J1',
 'DE000A1MMCC8',
 'DE000A1PHFF7',
 'DE000A1X3XX4',
 'DE000A288904',
 'DE000A2E4K43',
 'DE000A2GS401',
 'DE000A2GS5D8',
 'DE000A2LQ884',
 'DE000A2NB601',
 'DE000A2NBX80',
 'DE000A2TSL71',
 'DE000A2YN900',
 'DE000A3CNK42',
 'DE000A3E5D64',
 'DE000A3H2200',
 'DE000A3H2333',
 'DE000A3H3LL2',
 'DE000BASF111',
 'DE000BAY0017',
 'DE000BFB0019',
 'DE000CBK1001',
 'DE000DTR0CK8',
 'DE000DWS1007',
 'DE000ENAG999',
 'DE000ENER6Y0',
 'DE000EVNK013',
 'DE000FTG1111',
 'DE000HAG0005',
 'DE000JST4000',
 'DE000KBX1006',
 'DE000KC01000',
 'DE000KGX8881',
 'DE000KSAG888',
 'DE000LEG1110',
 'DE000NWRK013',
 'DE000PAH0038',
 'DE000PAT1AG3',
 'DE000PSM7770',
 'DE000SAFH001',
 'DE000SHA0159',
 'DE000SHL1006',
 'DE000STRA555',
 'DE000SYM9999',
 'DE000TLX1005',
 'DE000TRAT0N7',
 'DE000UNSE018',
 'DE000VTSC017',
 'DE000WACK012',
 'DE000WAF3001',
 'DE000WCH8881',
 'DE000ZAL1111',
 'DE000ZEAL241',
 'GB00BZ09BD16',
 'IE00BZ12WP82',
 'LU0061462528',
 'LU0775917882',
 'LU1066226637',
 'LU1250154413',
 'LU1673108939',
 'LU1704650164',
 'LU2333210958',
 'NL0000235190',
 'NL0009538784',
 'NL0012044747',
 'NL0012169213',
 'US00486H1059',
 'US00507V1098',
 'US00724F1012',
 'US0079031078',
 'US0090661010',
 'US0162551016',
 'US02079K1079',
 'US02079K3059',
 'US0231351067',
 'US0255371017',
 'US0258161092',
 'US0311621009',
 'US0326541051',
 'US03662Q1058',
 'US0378331005',
 'US0382221051',
 'US0463531089',
 'US0527691069',
 'US0530151036',
 'US0567521085',
 'US09062X1037',
 'US09075V1026',
 'US0970231058',
 'US09857L1089',
 'US11135F1012',
 'US1264081035',
 'US1273871087',
 'US1491231015',
 'US16119P1084',
 'US1667641005',
 'US17275R1023',
 'US1729081059',
 'US1912161007',
 'US1924461023',
 'US20030N1019',
 'US21037T1097',
 'US2172041061',
 'US22160K1051',
 'US22788C1053',
 'US23804L1035',
 'US2521311074',
 'US2546871060',
 'US2561631068',
 'US2567461080',
 'US2605571031',
 'US2786421030',
 'US2855121099',
 'US30303M1027',
 'US3119001044',
 'US3377381088',
 'US34959E1091',
 'US35137L1052',
 'US3755581036',
 'US38141G1040',
 'US4370761029',
 'US4385161066',
 'US45168D1046',
 'US4523271090',
 'US4581401001',
 'US4592001014',
 'US4612021034',
 'US46120E6023',
 'US46625H1005',
 'US47215P1066',
 'US4781601046',
 'US4824801009',
 'US49271V1008',
 'US5007541064',
 'US5128071082',
 'US5494981039',
 'US5500211090',
 'US5719032022',
 'US5738741041',
 'US57667L1070',
 'US5801351017',
 'US58733R1023',
 'US58933Y1055',
 'US5949181045',
 'US5950171042',
 'US5951121038',
 'US60770K1079',
 'US6092071058',
 'US61174X1090',
 'US64110L1061',
 'US64110W1027',
 'US6541061031',
 'US67066G1040',
 'US67103H1077',
 'US6792951054',
 'US6795801009',
 'US6937181088',
 'US6974351057',
 'US7043261079',
 'US70450Y1038',
 'US7134481081',
 'US7223041028',
 'US7427181091',
 'US7475251036',
 'US75886F1075',
 'US7782961038',
 'US79466L3024',
 'US81181C1045',
 'US82968B1035',
 'US83088M1027',
 'US8486371045',
 'US8552441094',
 'US8716071076',
 'US8725901040',
 'US88160R1014',
 'US8825081040',
 'US88579Y1010',
 'US89417E1091',
 'US91324P1021',
 'US92343E1029',
 'US92343V1044',
 'US92345Y1064',
 'US92532F1003',
 'US92556V1061',
 'US92826C8394',
 'US9311421039',
 'US9314271084',
 'US98138H1014',
 'US98389B1008',
 'US98980G1022',
 'US98980L1017',
 'USN070592100']


import pandas as pd

def extract_time_features(timestamp):
    week_number = timestamp.strftime('%U')
    day_of_year = timestamp.timetuple().tm_yday
    day_of_month = timestamp.day
    weekday = timestamp.strftime('%A')
    hour = timestamp.hour
    minute = timestamp.minute

    return {
        'Week': week_number,
        'DayOfYear': day_of_year,
        'DayOfMonth': day_of_month,
        'Weekday': weekday,
        'Hour': hour,
        'Minute': minute
    }

import yfinance as yf
import datetime

stock_symbols = stocklist

dcombined_df = pd.DataFrame()
lcombined_df = pd.DataFrame()


for symbol in stock_symbols:
# Symbol der Aktie (z.B. Apple)
    stock_symbol = symbol

    # Datum für die letzten 5 Jahre

    # Daten von Yahoo Finance abrufen

    end_date = datetime.datetime.today()
    start_date = end_date - datetime.timedelta(days=50)
    ldata = yf.download(stock_symbol, start=start_date, end=end_date)
    if not ldata.empty:
        print('Ldata', ldata)
        ldata[str(symbol)] = ldata['Close'].pct_change() * 100
    #ldata[str(symbol)] = ldata['Close']
        ldata.fillna(method='ffill').fillna(method='bfill')
    #print(ldata)


    end_date = datetime.datetime.today()
    start_date = end_date - datetime.timedelta(days=50)
    ddata = yf.download(stock_symbol, start=start_date, end=end_date, interval='5m')
    if not ddata.empty:
         ddata[str(symbol)] = ddata['Close'].pct_change() * 100
         #ddata[str(symbol)] = ddata['Close']
         # Die 'Close'-Spalte (Schlusskurse) verwenden und prozentuale Veränderung berechnen
         ddata.fillna(method='ffill').fillna(method='bfill')
         print(ddata.columns)

    #print(ddata)


    # Zeige die Daten mit den prozentualen Veränderungen an
    spalten = [0,5]
    print(ddata.columns)
    ddf = ddata.iloc[:, spalten]

    ddf.columns = [ str(symbol)+ '_Open', str(symbol) + '_proz']
    print(ddf)
    ldf = ldata.iloc[:, spalten]
    ldf.columns = [ str(symbol)+ '_Open', str(symbol) + '_proz']
    print(ldf)
    #new_column_name = str(symbol) + '_Close'
    #df.rename(columns={'Close': new_column_name}, inplace=True)

    dcombined_df = pd.concat([dcombined_df, ddf], axis=1)
    lcombined_df = pd.concat([lcombined_df, ldf], axis=1)


dcombined_df =  dcombined_df.fillna(method='pad').fillna(method='bfill')
lcombined_df =  lcombined_df.fillna(method='pad').fillna(method='bfill')

dcombined = dcombined_df.dropna(axis=1)
print(dcombined)
lcombined = lcombined_df.dropna(axis=1)
print(lcombined)





dcombined = dcombined.reset_index()
dcombined[['Woche','Jahrestag', 'Tag', 'Wochentag', 'Stunde' , 'Minute']] = dcombined['index'].apply(extract_time_features).apply(pd.Series)
#dcombined.to_csv('dcombined.csv')

dcombined.to_csv('data/dzwischen.csv')
del dcombined


lcombined = lcombined.reset_index()
lcombined[['Woche','Jahrestag', 'Tag', 'Wochentag', 'Stunde' , 'Minute']] = lcombined['Date'].apply(extract_time_features).apply(pd.Series)
#lcombined.to_csv('lcombined.csv')

lcombined.to_csv('data/lzwischen.csv')
del lcombined

old_df = pd.read_csv('data/dcombined.csv').sort_values(by = 'index')
new_df = pd.read_csv('data/dzwischen.csv')

# Finde neue Zeilen, die im neuen DataFrame vorhanden sind, aber nicht im alten DataFrame
new_rows = new_df[~new_df.isin(old_df.to_dict('list')).all(1)]

# Füge die neuen Zeilen zum alten DataFrame hinzu
updated_df = pd.concat([old_df, new_rows], ignore_index=True)
print(updated_df)
# Speichere den aktualisierten DataFrame in einer CSV-Datei
updated_df.to_csv('data/dcombined.csv', index=False)
del updated_df

old_df = pd.read_csv('data/lcombined.csv').sort_values(by = 'Date')

old_df = pd.read_csv('data/lcombined.csv')
new_df = pd.read_csv('data/lzwischen.csv')

# Finde neue Zeilen, die im neuen DataFrame vorhanden sind, aber nicht im alten DataFrame
new_rows = new_df[~new_df.isin(old_df.to_dict('list')).all(1)]

# Füge die neuen Zeilen zum alten DataFrame hinzu
updated_df = pd.concat([old_df, new_rows], ignore_index=True)


# Speichere den aktualisierten DataFrame in einer CSV-Datei
updated_df.to_csv('data/lcombined.csv', index=False)
#del updated_df

updated_df
updated_df.to_csv('data/lcombined.csv', index=False)



file_list = ['data/lcombined.csv']
for file in file_list:

# Load the CSV file into a DataFrame

    df = pd.read_csv(file)
    print(len(df))
    print(file)
    # Find and remove duplicate rows
    df.drop_duplicates(subset = 'Date' , inplace=True)
    print(len(df))

    # Save the DataFrame back to the CSV file without changing the structure
    df.to_csv(file, index=False)


file_list = ['data/dcombined.csv']
for file in file_list:

# Load the CSV file into a DataFrame

    df = pd.read_csv(file)
    print(len(df))
    print(file)
    # Find and remove duplicate rows
    df.drop_duplicates(subset = 'index' , inplace=True)
    print(len(df))

    # Save the DataFrame back to the CSV file without changing the structure
    df.to_csv(file, index=False)

import pandas as pd


df1 = pd.read_csv('data/lcombined.csv')
#print(df1)
df2 = pd.read_csv('data/dcombined.csv')

# Konvertiere die Spalte 'Jahrestag' in beiden DataFrames in das Datumsformat
#df1['Jahrestag'] = pd.to_datetime(df1['Jahrestag'])
#df2['Jahrestag'] = pd.to_datetime(df2['Jahrestag'])

# Iteriere über die gemeinsamen Spaltennamen außer 'Jahrestag'
common_columns = set(df1.columns).intersection(set(df2.columns)) - {'Jahrestag'}
for col in sorted(common_columns):
    grouped_df2 = df2.groupby('Jahrestag')[col].apply(list).reset_index()
    merged_df = df1.merge(grouped_df2, on='Jahrestag', how='left')
    merged_df.rename(columns={f'{col}_y': f'{col}_list'}, inplace=True)
    df1 = merged_df.copy()

df = df1
#print(df)
def umwandeln_in_stunden_und_minuten(datum_liste):
    neue_liste = []
    for datum in datum_liste:
        stunde_minute = pd.to_datetime(datum).strftime('%H:%M')
        neue_liste.append(stunde_minute)
    return neue_liste

# Anwenden der Funktion auf die Spalte
df['index_list'] = df['index_list'].apply(umwandeln_in_stunden_und_minuten)


list_spalten = df.filter(like='list')

start_uhrzeit = pd.to_datetime('9:15')

# Funktion, um den Index des höchsten Werts in einer Liste zu finden
def index_des_hoechsten_werts(lst):
    zeitchen = start_uhrzeit + pd.to_timedelta(lst.index(max(lst)) * 5, unit='minutes')
    return zeitchen.strftime('%H:%M')

def index_des_niedrigsten_werts(lst):
    zeitchen = start_uhrzeit + pd.to_timedelta(lst.index(min(lst)) * 5, unit='minutes')
    return zeitchen.strftime('%H:%M')

# Verwenden Sie apply, um den Index des höchsten Werts in jeder Spalte zu berechnen
index_df_hi = list_spalten.applymap(index_des_hoechsten_werts)
index_df_lo = list_spalten.applymap(index_des_niedrigsten_werts)

df1 = index_df_hi

df2 = index_df_lo


# Erstellen eines neuen DataFrames mit Tupeln in den Zellen für alle Spalten
combined_data = {}

for col in df1.columns:
    combined_data[col] = list(zip(df1[col], df2[col]))

index_df_high_lo = pd.DataFrame(combined_data)

print(index_df_high_lo)

def get_second_element_value_counts(column):
    first_elements = column.apply(lambda x: x[1])
    return first_elements.value_counts()
def get_first_element_value_counts(column):
    first_elements = column.apply(lambda x: x[0])
    return first_elements.value_counts()

# Value Counts für das erste Element der Tupel in allen Spalten berechnen
value_counts_per_column_high = index_df_high_lo.apply(get_first_element_value_counts).fillna(0)

# Ergebnis der Value Counts für jedes erste Element in jeder Spalte
#print(value_counts_per_column)

liste = value_counts_per_column_high.filter(like='proz_list').columns
#list_spalten = df.filter(like='list').columns
print(len(liste))
value_counts_per_column_high['highs'] = value_counts_per_column_high[liste].sum(axis=1) / (len(liste)* value_counts_per_column_high.iloc[:,1:].sum()[0])
print(value_counts_per_column_high)
value_counts_per_column_high.highs.plot()

# Value Counts für das erste Element der Tupel in allen Spalten berechnen
value_counts_per_column_low = index_df_high_lo.apply(get_second_element_value_counts).fillna(0)

# Ergebnis der Value Counts für jedes erste Element in jeder Spalte
#print(value_counts_per_column)

liste = value_counts_per_column_low.filter(like='proz_list').columns
#list_spalten = df.filter(like='list').columns
print(len(liste))

value_counts_per_column_low['lows'] = value_counts_per_column_low[liste].sum(axis=1) / (len(liste)* value_counts_per_column_low.iloc[:,1:].sum()[0])

#value_counts_per_column_low.lows.plot()
value_counts_per_column_low['highs'] = value_counts_per_column_high['highs']
print(value_counts_per_column_low)
value_counts_per_column_low[['highs','lows']].plot()

